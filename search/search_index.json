{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Flow-IQ! Flow-IQ is a toolkit designed to help NIH researchers migrate Nextflow bioinformatics workflows from the Biowulf HPC cluster to the cloud.","title":"Home"},{"location":"#welcome-to-flow-iq","text":"Flow-IQ is a toolkit designed to help NIH researchers migrate Nextflow bioinformatics workflows from the Biowulf HPC cluster to the cloud.","title":"Welcome to Flow-IQ!"},{"location":"explorer/","text":"Flow-IQ An interactive tool that maps Biowulf software modules and example datasets to their corresponding cloud-compatible Docker images and AWS S3 locations, helping users transition bioinformatics workflows from HPC to the cloud. Biowulf Module to Docker Image Builder Use the dropdown boxes below to locate the equivalent docker image for each Biowulf module. Biowulf Module [ select ] Version Please select a version Docker Image Copy waiting for selection\u2026 Example Data Builder Use the dropdown boxes below to locate example datasets for corresponding data types. Data Type [ select ] Additional Options Please select any additional options Data Locations waiting for selection\u2026 Description waiting for selection\u2026 // Docker Image Section $(function () { // Initiate clipboard.js new Clipboard('.prewrap > .btn'); // Step 1: Load the JSON mapping files and populate the first dropdown $.getJSON('flowiq_mapping.json', function (data) { const $biowulfDropdown = $('#all_modules'); const $versionDropdown = $('#version'); const $dockerImage = $('#docker_ref'); const $copyButton = $('#docker_ref_wrapper .btn'); Object.keys(data).forEach(key => { $biowulfDropdown.append($('<option>', { value: key, text: key })); }); // Populate the second dropdown when the first dropdown changes $biowulfDropdown.change(function () { const selectedModule = $(this).val(); // Reset the second dropdown and docker image reference $versionDropdown.empty().append('<option value=\"\">Please select a version</option>').prop('disabled', true); $dockerImage.html('<span class=\"text-muted\">waiting for selection&hellip;</span>'); $copyButton.prop('disabled', true); if (selectedModule && data[selectedModule]) { // Populate the second dropdown Object.keys(data[selectedModule]).forEach(key => { $versionDropdown.append($('<option>', { value: key, text: key })); }); $versionDropdown.prop('disabled', false); // Enable the second dropdown } }); // Update the docker image reference when the second dropdown changes $versionDropdown.change(function () { const selectedModule = $biowulfDropdown.val(); const selectedVersion = $(this).val(); if (selectedModule && selectedVersion && data[selectedModule][selectedVersion]) { // Set the docker image reference and enable the copy button $dockerImage.text(data[selectedModule][selectedVersion]); $copyButton.prop('disabled', false); } else { // Reset the docker image reference and disable the copy button $dockerImage.html('<span class=\"text-muted\">waiting for selection&hellip;</span>'); $copyButton.prop('disabled', true); } }); }).fail(function () { console.error('Error loading or parsing JSON'); }); }); //////////////////////////////////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////////////////////////////////// // Example Data Section $(function () { // Initiate clipboard.js new Clipboard('.prewrap > .btn'); // Step 1: Load the JSON mapping files and populate the first dropdown $.getJSON('test_data_mapping.json', function (data) { const $dataTypeDropdown = $('#all_data_types'); const $dataOptionDropdown = $('#data_options'); const $s3Location = $('#data_ref'); // const $copyButton = $('#data_ref_wrapper .btn'); const $dataDescription = $('#description'); Object.keys(data).forEach(key => { $dataTypeDropdown.append($('<option>', { value: key, text: key })); }); // Populate the second dropdown when the first dropdown changes $dataTypeDropdown.change(function () { const selectedData = $(this).val(); // Reset the second dropdown and data options reference $dataOptionDropdown.empty().append('<option value=\"\">Please select a version</option>').prop('disabled', true); $s3Location.html('<span class=\"text-muted\">waiting for selection&hellip;</span>'); // $copyButton.prop('disabled', true); $dataDescription.html('<span class=\"text-muted\">waiting for selection&hellip;</span>').prop('disabled', true) if (selectedData && data[selectedData]) { $dataOptionDropdown.prop('disabled', false); // Enable the second dropdown // Populate the second dropdown Object.keys(data[selectedData]).forEach(key => { if (key !== \"Description\") { $dataOptionDropdown.append($('<option>', { value: key, text: key })); } }); } }); // Update the Data Locations + Description when the second dropdown changes $dataOptionDropdown.change(function () { const selectedData = $dataTypeDropdown.val(); const selectedOption = $(this).val(); // Set the Data Locations and Description Boxes if (selectedData && selectedOption && data[selectedData][selectedOption]) { // Parse the array of values const arrayValues = data[selectedData][selectedOption]; // Generate a code block if the data is an array if (Array.isArray(arrayValues)) { const formattedValues = arrayValues.join('\\n'); $s3Location.text(formattedValues); // Add array to code block } else { $s3Location.text(arrayValues); // For non-array data, just display the value } // $copyButton.prop('disabled', false); // Display the description $dataDescription.text(data[selectedData][\"Description\"]); } else { // Reset Data Locations $s3Location.html('<span class=\"text-muted\">waiting for selection&hellip;</span>'); // $copyButton.prop('disabled', true); } }); }).fail(function () { console.error('Error loading or parsing JSON'); }); });","title":"Flow-IQ"},{"location":"explorer/#flow-iq","text":"An interactive tool that maps Biowulf software modules and example datasets to their corresponding cloud-compatible Docker images and AWS S3 locations, helping users transition bioinformatics workflows from HPC to the cloud.","title":"Flow-IQ"},{"location":"getting-started/about-flowiq/","text":"About Flow-IQ Flow-IQ is a toolkit designed to help researchers transition bioinformatics workflows from traditional HPC environments like Biowulf to the AWS cloud. Key Features An interactive toolkit for mapping Biowulf environment modules to matching Docker images, along with curated links to cloud-accessible datasets (e.g., AWS Open Data equivalents of Biowulf iGenomes). Guides and demos on using nf-core/tools and custom linters to verify that pipelines are cloud-ready. Step-by-step instructions for building custom pipelines from nf-core modules that run on both HPC and AWS environments. How It's Organized The User Guide walks you through each step of building a custom Nextflow pipeline using nf-core modules. You'll learn how to assemble your pipeline, make it portable, and prepare it for cloud deployment using linters and the Flow-IQ Explorer to locate compatible data and containers. This resource was developed by Wendy Wong , a bioinformatician at the National Cancer Institute - Division of Cancer Epidemiology & Genetics , and Jesse Marks, a bioinformatics software engineer at RTI International - GenOmics and Translational Research Center.","title":"About Flow-IQ"},{"location":"getting-started/about-flowiq/#about-flow-iq","text":"Flow-IQ is a toolkit designed to help researchers transition bioinformatics workflows from traditional HPC environments like Biowulf to the AWS cloud.","title":"About Flow-IQ"},{"location":"getting-started/about-flowiq/#key-features","text":"An interactive toolkit for mapping Biowulf environment modules to matching Docker images, along with curated links to cloud-accessible datasets (e.g., AWS Open Data equivalents of Biowulf iGenomes). Guides and demos on using nf-core/tools and custom linters to verify that pipelines are cloud-ready. Step-by-step instructions for building custom pipelines from nf-core modules that run on both HPC and AWS environments.","title":"Key Features"},{"location":"getting-started/about-flowiq/#how-its-organized","text":"The User Guide walks you through each step of building a custom Nextflow pipeline using nf-core modules. You'll learn how to assemble your pipeline, make it portable, and prepare it for cloud deployment using linters and the Flow-IQ Explorer to locate compatible data and containers. This resource was developed by Wendy Wong , a bioinformatician at the National Cancer Institute - Division of Cancer Epidemiology & Genetics , and Jesse Marks, a bioinformatics software engineer at RTI International - GenOmics and Translational Research Center.","title":"How It's Organized"},{"location":"getting-started/what-you-should-know/","text":"Before You Start: What You Should Know If you\u2019re new to any of the core technologies used in Flow-IQ, here are some helpful resources: Nextflow: Nextflow A workflow system for creating scalable, portable, and reproducible workflows. Consider taking the Nextflow training tutorial to get up to speed. nf-core: A community effort to collect a curated set of analysis pipelines built with Nextflow. Also see \"What is nf-core?\" . Linters: Tools that check code for errors or non-compliance with standards. Also see this Wikipedia article on linting. Containers: Bundles your app and its dependencies so it runs the same everywhere. Also see the Docker Container Guide .","title":"Before You Start: What You Should Know"},{"location":"getting-started/what-you-should-know/#before-you-start-what-you-should-know","text":"If you\u2019re new to any of the core technologies used in Flow-IQ, here are some helpful resources: Nextflow: Nextflow A workflow system for creating scalable, portable, and reproducible workflows. Consider taking the Nextflow training tutorial to get up to speed. nf-core: A community effort to collect a curated set of analysis pipelines built with Nextflow. Also see \"What is nf-core?\" . Linters: Tools that check code for errors or non-compliance with standards. Also see this Wikipedia article on linting. Containers: Bundles your app and its dependencies so it runs the same everywhere. Also see the Docker Container Guide .","title":"Before You Start: What You Should Know"},{"location":"troubleshooting/troubleshooting-and-tips/","text":"Seqera AI: Bioinformatics Agent for Nextflow Seqera is the company behind Nextflow. They have built an Bioinformatics AI agent trained specifically for Nextflow. You can use this to accelerate your workflow building and reduce the time you spend troubleshooting so you can spend less time on the undifferentiated work and more time crafting your bioinformatics pipeline. Here are the steps to using it: Visit Ask-AI by Seqera. Sign in using your GitHub or Google account. Type a question or paste a prompt into the input bar to get help instantly. \ud83d\udca1 You can also try the Nextflow VS Code Extension for inline AI support while coding.","title":"Seqera AI: Bioinformatics Agent for Nextflow"},{"location":"troubleshooting/troubleshooting-and-tips/#seqera-ai-bioinformatics-agent-for-nextflow","text":"Seqera is the company behind Nextflow. They have built an Bioinformatics AI agent trained specifically for Nextflow. You can use this to accelerate your workflow building and reduce the time you spend troubleshooting so you can spend less time on the undifferentiated work and more time crafting your bioinformatics pipeline. Here are the steps to using it: Visit Ask-AI by Seqera. Sign in using your GitHub or Google account. Type a question or paste a prompt into the input bar to get help instantly. \ud83d\udca1 You can also try the Nextflow VS Code Extension for inline AI support while coding.","title":"Seqera AI: Bioinformatics Agent for Nextflow"},{"location":"user-guide/","text":"Custom Pipeline Development This section walks you through how to build a custom pipeline using an nf-core module, test it on the Biowulf HPC, and adapt it for AWS HealthOmics with Flow-IQ. We'll follow a real-world example and explain each step, focusing on what needs to happen and why. Example Scenario Suppose you\u2019re working with the nf-core Sarek pipeline and then have an idea for an analysis using one step of the pipeline. In particular, you want to build a custom pipeline for your analysis using the Manta germline module. This guide will show you how to: - Extract and reuse the module - Build a minimal pipeline around it - Test it locally (e.g., on Biowulf HPC) - Prepare and deploy it to AWS HealthOmics using Flow-IQ","title":"Custom Pipeline Development"},{"location":"user-guide/#custom-pipeline-development","text":"This section walks you through how to build a custom pipeline using an nf-core module, test it on the Biowulf HPC, and adapt it for AWS HealthOmics with Flow-IQ. We'll follow a real-world example and explain each step, focusing on what needs to happen and why.","title":"Custom Pipeline Development"},{"location":"user-guide/#example-scenario","text":"Suppose you\u2019re working with the nf-core Sarek pipeline and then have an idea for an analysis using one step of the pipeline. In particular, you want to build a custom pipeline for your analysis using the Manta germline module. This guide will show you how to: - Extract and reuse the module - Build a minimal pipeline around it - Test it locally (e.g., on Biowulf HPC) - Prepare and deploy it to AWS HealthOmics using Flow-IQ","title":"Example Scenario"},{"location":"user-guide/step1-locate-module/","text":"Step 1: Locate the Module Let\u2019s say you\u2019ve seen Manta used within the Sarek pipeline and want to use the manta_germline module in a standalone workflow. You\u2019re not modifying Sarek\u2014you\u2019re building something new, based on a reusable module.","title":"Step 1: Locate the Module"},{"location":"user-guide/step1-locate-module/#step-1-locate-the-module","text":"Let\u2019s say you\u2019ve seen Manta used within the Sarek pipeline and want to use the manta_germline module in a standalone workflow. You\u2019re not modifying Sarek\u2014you\u2019re building something new, based on a reusable module.","title":"Step 1: Locate the Module"},{"location":"user-guide/step2-understanding-nfcore-modules/","text":"Step 2: Understand How nf-core Modules Work While it might seem like you can just run: nf-core modules install manta/germline ...it's not quite that simple. nf-core modules are reusable building blocks\u2013not pipelines. You need a proper Nextflow pipeline project to plug the module into. Without this, you'll run into errors because modules can't run on their own. They need a pipeline \"framework\" to run within else you will get an error like you see below. So let's move on to the next step and see what we need to do first.","title":"Step 2: Understand How nf-core Modules Work"},{"location":"user-guide/step2-understanding-nfcore-modules/#step-2-understand-how-nf-core-modules-work","text":"While it might seem like you can just run: nf-core modules install manta/germline ...it's not quite that simple. nf-core modules are reusable building blocks\u2013not pipelines. You need a proper Nextflow pipeline project to plug the module into. Without this, you'll run into errors because modules can't run on their own. They need a pipeline \"framework\" to run within else you will get an error like you see below. So let's move on to the next step and see what we need to do first.","title":"Step 2: Understand How nf-core Modules Work"},{"location":"user-guide/step3-create-pipeline/","text":"Step 3: Create a pipeline We\u2019ll use the nf-core tool to scaffold a new custom pipeline. You\u2019ll be walked through an interactive prompt to fill in: - Basic info (name, description) - Default template features (we\u2019ll use the defaults) - Location for the new pipeline - Optional GitHub repo (you can skip this for now and add one later)","title":"Step 3: Create a pipeline"},{"location":"user-guide/step3-create-pipeline/#step-3-create-a-pipeline","text":"We\u2019ll use the nf-core tool to scaffold a new custom pipeline. You\u2019ll be walked through an interactive prompt to fill in: - Basic info (name, description) - Default template features (we\u2019ll use the defaults) - Location for the new pipeline - Optional GitHub repo (you can skip this for now and add one later)","title":"Step 3: Create a pipeline"},{"location":"user-guide/step4-install-manta-module/","text":"Step 4: Install the Manta Module Inside your new pipeline folder, run: nf-core modules install manta/germline This will add the module to your project and update your modules.json . \ud83d\udca1 Notice at the end of the GIF: it shows how to include the module in your workflow using the following line: include { MANTA_GERMLINE } from `../modules/nf-core/manta/germline/main` Now let's build a simple pipeline using this Manta module.","title":"Step 4: Install the Manta Module"},{"location":"user-guide/step4-install-manta-module/#step-4-install-the-manta-module","text":"Inside your new pipeline folder, run: nf-core modules install manta/germline This will add the module to your project and update your modules.json . \ud83d\udca1 Notice at the end of the GIF: it shows how to include the module in your workflow using the following line: include { MANTA_GERMLINE } from `../modules/nf-core/manta/germline/main` Now let's build a simple pipeline using this Manta module.","title":"Step 4: Install the Manta Module"},{"location":"user-guide/step5-build-a-one-step-workflow/","text":"Step 5: Build a One-Step Workflow When you first enter your newly created pipeline directory, you'll see a few files and directories generated by nf-core : For now, we'll focus on main.nf , which is the core Nextflow script where we'll integrate the Manta module into our custom pipeline. Understanding main.nf The first few lines of main.nf outline the pipeline's basic structure: /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nci-dceg/flowiq ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Github : https://github.com/nci-dceg/flowiq ---------------------------------------------------------------------------------------- */ /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPORT FUNCTIONS / MODULES / SUBWORKFLOWS / WORKFLOWS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ include { FLOWIQ } from './workflows/flowiq' include { PIPELINE_INITIALISATION } from './subworkflows/local/utils_nfcore_flowiq_pipeline' include { PIPELINE_COMPLETION } from './subworkflows/local/utils_nfcore_flowiq_pipeline' include { getGenomeAttribute } from './subworkflows/local/utils_nfcore_flowiq_pipeline' Locate the section labeled IMPORT FUNCTIONS / MODULES / SUBWORKFLOWS / WORKFLOWS . This is where you'll add the include statement for the Manta module you installed in Step 4. Add it directly below the existing include lines. This statement allows your pipeline to reference the module, which lives in the modules/ directory at the root of your project. Understanding nextflow.config Now, let's look at the configuration file at the root of your pipeline: nextflow.config . This file is key for: Setting pipeline-wide parameters. Defining default values used throughout the workflow. Referencing additional config files (like conf/igenomes.config ). For example, you might see this line in the params block: igenomes_base = 's3://ngi-igenomes/igenome' This line specifies the base path for AWS-hosted reference genome files. But how does your pipeline use it? In main.nf , you might find a line like: params.fasta = getGenomeAttribute('fasta') This line leverages the getGenomeAttribute function, which was automatically included earlier. It dynamically builds the path to the correct reference file based on the genome build you've selected. For instance, if params.genome = 'GRCh37' , the FASTA file path would resolve to: s3://ngi-igenomes/igenomes/Homo_sapiens/Ensembl/GRCh37/Sequence/WholeGenomeFasta/genome.fa You can verify this logic by checking the contents of conf/igenomes.config , which supports various igenomes references: Understanding Manta's Requirements To effectively use the Manta module, it's crucial to understand its expected inputs and outputs. The best place to find this information is the nf-core module page: \ud83d\udd17 https://nf-co.re/modules/manta_germline/ On this page, you'll find: A module description. Detailed input/output channel definitions. Parameter requirements. A link to the module's source code on GitHub. The corresponding GitHub repository is: \ud83d\udd17 https://github.com/nf-core/modules/tree/master/modules/nf-core/manta/germline \ud83d\udca1 Tip: Each nf-core module page also includes links to more in-depth documentation at the bottom. All the information on the module's webpage is backed by its meta.yml file, located at modules/nf-core/manta/germline/meta.yml . Let's dive into the input requirements for Manta as outlined on its webpage. You'll notice it specifies four distinct input blocks , some grouped under meta , meta2 , and meta3 variables, which bundle related values. Here's a breakdown of each input block: BAM/CRAM/SAM files + their indexes: These are the aligned sequencing reads ( .bam or .cram files) that Manta will analyze. Each input alignment file (e.g., .bam ) must have a corresponding index file ( .bai for BAM, .crai for CRAM). For joint calling (analyzing multiple samples together), you can pass in multiple files. Target regions (optional, for exome or targeted panels): A BED file, often compressed ( .bed.gz ), instructs Manta to limit variant calling to specific genomic regions (like exons). It requires an accompanying .tbi index file for quick access. Reference genome: FASTA file + index: The fasta file (grouped with meta2 ) is the reference genome. Your generated main.nf already has a section for managing this. The fai file (grouped with meta3 ) is its corresponding FASTA index. Optional config file: This allows you to customize Manta 's settings by providing your own configuration file. \u2753 What Do I Need to Run Manta? To successfully run the Manta module, you'll need the following input files: \u2705 Required One .bam or .cram file per sample, paired with its .bai or .crai index. A reference genome in .fa format, along with its .fa.fai index. \ud83d\udfe1 Optional A BED file ( .bed.gz ) for targeted calling, plus its .tbi index. A custom Manta configuration file. Where to Find Example Input Files The best place to start is by examining the nf-core module test file: modules/nf-core/manta/germline/tests/main.nf.test Every nf-core module includes a main.nf.test file. This file defines how to test the module using small example datasets, typically sourced from the nf-core/test-datasets repository: \ud83d\udd17 https://github.com/nf-core/test-datasets You can also find a detailed guide on using this data here: \ud83d\udd17 https://github.com/nf-core/test-datasets/blob/master/docs/USE_EXISTING_DATA.md About the Test Datasets The test-datasets repository has a special modules branch specifically for individual module tests: This branch of the nf-core/test-datasets repository contains all data used for the individual module tests. We'll use this branch to test our Manta module. What main.nf.test Expects If you open main.nf.test , you'll notice it references inputs like this: file(params.modules_testdata_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram', checkIfExists: true) This indicates we need to define the base path params.modules_testdata_base_path in a configuration file. \ufe0fSetting Up manta_input_data_base_path Instead of modifying the main nextflow.config , it's generally cleaner to add this to a specific profile, such as the testing profile. In nextflow.config , you'll find a test profile: test { includeConfig 'conf/test.config' } Edit the conf/test.config file and add the following: // Input data // nf-core: Specify the paths to your test data from the test-datasets repo modules_testdata_base_path = 'https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/' This approach leverages the GitHub raw URL, allowing Nextflow to directly access the files. Running the Module Test Once your configuration is set, you can run the test using the nf-test tool (a framework for testing Nextflow pipelines and modules): nf-core modules test manta/germline -profile conda,test Now that we've seen how to test the Manta module using nf-core test data, we understand its required inputs, where to find example datasets, and how to configure them using Nextflow. Let's apply that knowledge by modifying our main.nf script to build our own pipeline. Integrating Manta into main.nf From examining and running the main.nf.test script for Manta , we can see that for the simplest test case ( test(\"human - cram\") ), there are four main input blocks. We'll model our main.nf after these. First, let's look at the sample input block from the test: input[0] = [ [ id:'test'], // meta map file(params.modules_testdata_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram', checkIfExists: true), file(params.modules_testdata_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram.crai', checkIfExists: true), [],[] ] This represents a single sample CRAM file and its index, with no BED or BED index specified. Next, we have the reference FASTA input sections: // fasta input[1] = [ [id:'genome'],file(params.modules_testdata_base_path + 'genomics/homo_sapiens/genome/genome.fasta', checkIfExists: true)] // fai input[2] = [ [id:'genome'],file(params.modules_testdata_base_path + 'genomics/homo_sapiens/genome/genome.fasta.fai', checkIfExists: true)] Followed by the config input: // config input[3] = Channel.of(\"[manta]\", \"enableRemoteReadRetrievalForInsertionsInGermlineCallingModes = 0\") .collectFile(name:\"manta_options.ini\", newLine:true) We'll discuss how to best organize the workflow later, but for now, let's reuse these inputs in our pipeline and add them to the main.nf script at the root of your repository. Defining Input Data Paths The first section you'll notice in main.nf after the import statements is GENOME PARAMETER VALUES . Here, nf-core has pre-populated the line params.fasta = getGenomeAttribute('fasta') . The Manta module's second input block is for the FASTA and its corresponding index file. While this getGenomeAttribute function is excellent for production runs with whole genome FASTA files from igenomes (see the dropdown below for details), for this tutorial, we'll stick with the simple example file used in the Manta test cases. Using Whole Genome FASTA File The `getGenomeAttribute` function (`def getGenomeAttribute(attribute)`) is designed to fetch genome-specific attributes. As you can see from its definition (which you can find using `git grep getGenomeAttribute`): def getGenomeAttribute(attribute) { if (params.genomes && params.genome && params.genomes.containsKey(params.genome)) { if (params.genomes[ params.genome ].containsKey(attribute)) { return params.genomes[ params.genome ][ attribute ] } } return null } This function expects two parameters, `genomes` and `genome`, to be defined. In your `nextflow.config`, you'll already find a section for references: // References genome = null igenomes_base = 's3://ngi-igenomes/igenomes/' This allows you to use `igenomes` references on AWS S3. To understand what to set for `genome`, you can consult `config/igenomes.config`, which lists various reference paths. For this example pipeline, we'll match the `Manta` test cases exactly. When we ran our Manta module test, we added the following to conf/test.config : // Input data // TODO nf-core: Specify the paths to your test data on nf-core/test-datasets modules_testdata_base_path = 'https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/' Let's adapt this parameter for our main configuration file, nextflow.config , by renaming it and adding it to the params scope. Paste the following: // Input options manta_input_data_base_path = 'https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/' Creating Input Channels in main.nf Now, let's update our main.nf script by adding our inputs as channels . Nextflow is built on a dataflow programming model, where processes communicate through channels, which are fundamental for moving data through your pipeline. You can learn more about them here: \ud83d\udd17 https://www.nextflow.io/docs/latest/channel.html Add the following CHANNEL DEFINITIONS section to your main.nf script: /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CHANNEL DEFINITIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ // Channel for the first input: tuple val(meta), path(input), path(index), path(target_bed), path(target_bed_tbi) Channel.from([ tuple( [ id: 'test' ], // meta file(params.manta_input_data_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram', checkIfExists: true), // input (CRAM) file(params.manta_input_data_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram.crai', checkIfExists: true), // index (CRAI) [], // target_bed (empty list, as no BED is used for this basic test) [] // target_bed_tbi (empty list) ) ]).set { manta_main_input_ch } // This channel matches the first input signature // Channel for the second input: tuple val(meta2), path(fasta) Channel.from([ tuple( [ id: 'genome' ], // meta2 file(params.manta_input_data_base_path + 'genomics/homo_sapiens/genome/genome.fasta', checkIfExists: true) // fasta ) ]).set { manta_fasta_ch } // Channel for the third input: tuple val(meta3), path(fai) Channel.from([ tuple( [ id: 'genome' ], // meta3 file(params.manta_input_data_base_path + 'genomics/homo_sapiens/genome/genome.fasta.fai', checkIfExists: true) // fai ) ]).set { manta_fai_ch } // Channel for the fourth input: path(config) Channel .of(\"[manta]\", \"enableRemoteReadRetrievalForInsertionsInGermlineCallingModes = 0\") .collectFile(name: \"manta_options.ini\", newLine: true) .set { manta_config_ch } Essentially, we're taking the input syntax from the tests/main.nf.test script and wrapping these inputs in Nextflow channels. Calling the Manta Module With the input channels defined, we can now call our module within the main workflow: /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ NAMED WORKFLOWS FOR PIPELINE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ workflow NCIDCEG_FLOWIQ { main: // Call MANTA_GERMLINE, feeding the channels in the correct order // as defined in the module's 'input:' block MANTA_GERMLINE( manta_main_input_ch, // Corresponds to `tuple val(meta), path(input), path(index), path(target_bed), path(target_bed_tbi)` manta_fasta_ch, // Corresponds to `tuple val(meta2), path(fasta)` manta_fai_ch, // Corresponds to `tuple val(meta3), path(fai)` manta_config_ch // Corresponds to `path(config)` ) } /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RUN MAIN WORKFLOW ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ workflow { main: NCIDCEG_FLOWIQ() } /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ THE END ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ Running Your Pipeline Now that your main.nf is updated, you can test the pipeline: nextflow run main.nf -profile conda You've now created a minimal pipeline that successfully runs the Manta module! Finishing Touches: Organizing Your Pipeline Now that we have a working pipeline, let's take a moment to clean things up and follow nf-core best practices. Open your main.nf script. Near the top, you'll see a section like this: /******************************************************************************************************** IMPORT FUNCTIONS / MODULES / SUBWORKFLOWS / WORKFLOWS ********************************************************************************************************/ include { FLOWIQ } from './workflows/flowiq' This line tells us that the pipeline is importing a workflow definition from the flowiq script located in the workflows/ directory. \ud83d\udc49 The nf-core convention is to keep the root main.nf minimal and delegate all workflow logic to these included scripts. This significantly improves readability, scalability, and maintainability as your pipeline grows. Since we've added the Manta module to our pipeline, we should also relocate its include statement: include { MANTA_GERMLINE } from '../modules/nf-core/manta/germline' This import should live in workflows/flowiq.nf instead of main.nf , because that's where the logic using it will reside. In short: keep main.nf tidy and focused on high-level orchestration, and let flowiq.nf handle the details of your workflow. Once you've done that, your pipeline will not only run correctly but also follow a clean, modular design that's easier to share and maintain. Connecting Modules: Emitting Outputs Finally, let's look at how to connect modules in a multi-step pipeline where you need to use the output from one module as input to another. The MANTA_GERMLINE module produces several outputs, as defined in its main.nf script. To make these outputs available to other processes or for further use in your workflow, you need to emit them from your workflow. Here's how you can do that within your FLOWIQ workflow: workflow FLOWIQ { main: MANTA_GERMLINE( manta_main_input_ch, // Corresponds to `tuple val(meta), path(input), path(index), path(target_bed), path(target_bed_tbi)` manta_fasta_ch, // Corresponds to `tuple val(meta2), path(fasta)` manta_fai_ch, // Corresponds to `tuple val(meta3), path(fai)` manta_config_ch // Corresponds to `path(config)` ) emit: candidate_small_indels_vcf = MANTA_GERMLINE.out.candidate_small_indels_vcf candidate_small_indels_vcf_tbi = MANTA_GERMLINE.out.candidate_small_indels_vcf_tbi candidate_sv_vcf = MANTA_GERMLINE.out.candidate_sv_vcf candidate_sv_vcf_tbi = MANTA_GERMLINE.out.candidate_sv_vcf_tbi diploid_sv_vcf = MANTA_GERMLINE.out.diploid_sv_vcf diploid_sv_vcf_tbi = MANTA_GERMLINE.out.diploid_sv_vcf_tbi versions = MANTA_GERMLINE.out.versions } We obtain the names of the output variables directly from the output: block of the manta/germline/main.nf script (e.g., emit: candidate_small_indels_vcf ). These are then assigned to new variables within the emit block of our FLOWIQ workflow. To verify that everything is working as expected, you can include a simple line in your workflow to print the contents of an output channel: candidate_small_indels_vcf.view() You can also inspect the Directed Acyclic Graph (DAG) generated in the output directory specified by params.outdir in nextflow.config . It provides a visual overview of the data flow between processes. For example: This helps confirm the structure and behavior of your pipeline by showing how data moves through each channel. Note: The main.nf and workflows/flowiq.nf scripts are included in this repository for your reference \u2014 in fact, we included the entire pipeline repository in nci-dceg-flowiq .","title":"Step 5: Build a One-Step Workflow"},{"location":"user-guide/step5-build-a-one-step-workflow/#step-5-build-a-one-step-workflow","text":"When you first enter your newly created pipeline directory, you'll see a few files and directories generated by nf-core : For now, we'll focus on main.nf , which is the core Nextflow script where we'll integrate the Manta module into our custom pipeline.","title":"Step 5: Build a One-Step Workflow"},{"location":"user-guide/step5-build-a-one-step-workflow/#understanding-mainnf","text":"The first few lines of main.nf outline the pipeline's basic structure: /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nci-dceg/flowiq ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Github : https://github.com/nci-dceg/flowiq ---------------------------------------------------------------------------------------- */ /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ IMPORT FUNCTIONS / MODULES / SUBWORKFLOWS / WORKFLOWS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ include { FLOWIQ } from './workflows/flowiq' include { PIPELINE_INITIALISATION } from './subworkflows/local/utils_nfcore_flowiq_pipeline' include { PIPELINE_COMPLETION } from './subworkflows/local/utils_nfcore_flowiq_pipeline' include { getGenomeAttribute } from './subworkflows/local/utils_nfcore_flowiq_pipeline' Locate the section labeled IMPORT FUNCTIONS / MODULES / SUBWORKFLOWS / WORKFLOWS . This is where you'll add the include statement for the Manta module you installed in Step 4. Add it directly below the existing include lines. This statement allows your pipeline to reference the module, which lives in the modules/ directory at the root of your project.","title":"Understanding main.nf"},{"location":"user-guide/step5-build-a-one-step-workflow/#understanding-nextflowconfig","text":"Now, let's look at the configuration file at the root of your pipeline: nextflow.config . This file is key for: Setting pipeline-wide parameters. Defining default values used throughout the workflow. Referencing additional config files (like conf/igenomes.config ). For example, you might see this line in the params block: igenomes_base = 's3://ngi-igenomes/igenome' This line specifies the base path for AWS-hosted reference genome files. But how does your pipeline use it? In main.nf , you might find a line like: params.fasta = getGenomeAttribute('fasta') This line leverages the getGenomeAttribute function, which was automatically included earlier. It dynamically builds the path to the correct reference file based on the genome build you've selected. For instance, if params.genome = 'GRCh37' , the FASTA file path would resolve to: s3://ngi-igenomes/igenomes/Homo_sapiens/Ensembl/GRCh37/Sequence/WholeGenomeFasta/genome.fa You can verify this logic by checking the contents of conf/igenomes.config , which supports various igenomes references:","title":"Understanding nextflow.config"},{"location":"user-guide/step5-build-a-one-step-workflow/#understanding-mantas-requirements","text":"To effectively use the Manta module, it's crucial to understand its expected inputs and outputs. The best place to find this information is the nf-core module page: \ud83d\udd17 https://nf-co.re/modules/manta_germline/ On this page, you'll find: A module description. Detailed input/output channel definitions. Parameter requirements. A link to the module's source code on GitHub. The corresponding GitHub repository is: \ud83d\udd17 https://github.com/nf-core/modules/tree/master/modules/nf-core/manta/germline \ud83d\udca1 Tip: Each nf-core module page also includes links to more in-depth documentation at the bottom. All the information on the module's webpage is backed by its meta.yml file, located at modules/nf-core/manta/germline/meta.yml . Let's dive into the input requirements for Manta as outlined on its webpage. You'll notice it specifies four distinct input blocks , some grouped under meta , meta2 , and meta3 variables, which bundle related values. Here's a breakdown of each input block: BAM/CRAM/SAM files + their indexes: These are the aligned sequencing reads ( .bam or .cram files) that Manta will analyze. Each input alignment file (e.g., .bam ) must have a corresponding index file ( .bai for BAM, .crai for CRAM). For joint calling (analyzing multiple samples together), you can pass in multiple files. Target regions (optional, for exome or targeted panels): A BED file, often compressed ( .bed.gz ), instructs Manta to limit variant calling to specific genomic regions (like exons). It requires an accompanying .tbi index file for quick access. Reference genome: FASTA file + index: The fasta file (grouped with meta2 ) is the reference genome. Your generated main.nf already has a section for managing this. The fai file (grouped with meta3 ) is its corresponding FASTA index. Optional config file: This allows you to customize Manta 's settings by providing your own configuration file.","title":"Understanding Manta's Requirements"},{"location":"user-guide/step5-build-a-one-step-workflow/#what-do-i-need-to-run-manta","text":"To successfully run the Manta module, you'll need the following input files:","title":"\u2753 What Do I Need to Run Manta?"},{"location":"user-guide/step5-build-a-one-step-workflow/#required","text":"One .bam or .cram file per sample, paired with its .bai or .crai index. A reference genome in .fa format, along with its .fa.fai index.","title":"\u2705 Required"},{"location":"user-guide/step5-build-a-one-step-workflow/#optional","text":"A BED file ( .bed.gz ) for targeted calling, plus its .tbi index. A custom Manta configuration file.","title":"\ud83d\udfe1 Optional"},{"location":"user-guide/step5-build-a-one-step-workflow/#where-to-find-example-input-files","text":"The best place to start is by examining the nf-core module test file: modules/nf-core/manta/germline/tests/main.nf.test Every nf-core module includes a main.nf.test file. This file defines how to test the module using small example datasets, typically sourced from the nf-core/test-datasets repository: \ud83d\udd17 https://github.com/nf-core/test-datasets You can also find a detailed guide on using this data here: \ud83d\udd17 https://github.com/nf-core/test-datasets/blob/master/docs/USE_EXISTING_DATA.md","title":"Where to Find Example Input Files"},{"location":"user-guide/step5-build-a-one-step-workflow/#about-the-test-datasets","text":"The test-datasets repository has a special modules branch specifically for individual module tests: This branch of the nf-core/test-datasets repository contains all data used for the individual module tests. We'll use this branch to test our Manta module.","title":"About the Test Datasets"},{"location":"user-guide/step5-build-a-one-step-workflow/#what-mainnftest-expects","text":"If you open main.nf.test , you'll notice it references inputs like this: file(params.modules_testdata_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram', checkIfExists: true) This indicates we need to define the base path params.modules_testdata_base_path in a configuration file.","title":"What main.nf.test Expects"},{"location":"user-guide/step5-build-a-one-step-workflow/#setting-up-manta_input_data_base_path","text":"Instead of modifying the main nextflow.config , it's generally cleaner to add this to a specific profile, such as the testing profile. In nextflow.config , you'll find a test profile: test { includeConfig 'conf/test.config' } Edit the conf/test.config file and add the following: // Input data // nf-core: Specify the paths to your test data from the test-datasets repo modules_testdata_base_path = 'https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/' This approach leverages the GitHub raw URL, allowing Nextflow to directly access the files.","title":"\ufe0fSetting Up manta_input_data_base_path"},{"location":"user-guide/step5-build-a-one-step-workflow/#running-the-module-test","text":"Once your configuration is set, you can run the test using the nf-test tool (a framework for testing Nextflow pipelines and modules): nf-core modules test manta/germline -profile conda,test Now that we've seen how to test the Manta module using nf-core test data, we understand its required inputs, where to find example datasets, and how to configure them using Nextflow. Let's apply that knowledge by modifying our main.nf script to build our own pipeline.","title":"Running the Module Test"},{"location":"user-guide/step5-build-a-one-step-workflow/#integrating-manta-into-mainnf","text":"From examining and running the main.nf.test script for Manta , we can see that for the simplest test case ( test(\"human - cram\") ), there are four main input blocks. We'll model our main.nf after these. First, let's look at the sample input block from the test: input[0] = [ [ id:'test'], // meta map file(params.modules_testdata_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram', checkIfExists: true), file(params.modules_testdata_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram.crai', checkIfExists: true), [],[] ] This represents a single sample CRAM file and its index, with no BED or BED index specified. Next, we have the reference FASTA input sections: // fasta input[1] = [ [id:'genome'],file(params.modules_testdata_base_path + 'genomics/homo_sapiens/genome/genome.fasta', checkIfExists: true)] // fai input[2] = [ [id:'genome'],file(params.modules_testdata_base_path + 'genomics/homo_sapiens/genome/genome.fasta.fai', checkIfExists: true)] Followed by the config input: // config input[3] = Channel.of(\"[manta]\", \"enableRemoteReadRetrievalForInsertionsInGermlineCallingModes = 0\") .collectFile(name:\"manta_options.ini\", newLine:true) We'll discuss how to best organize the workflow later, but for now, let's reuse these inputs in our pipeline and add them to the main.nf script at the root of your repository.","title":"Integrating Manta into main.nf"},{"location":"user-guide/step5-build-a-one-step-workflow/#defining-input-data-paths","text":"The first section you'll notice in main.nf after the import statements is GENOME PARAMETER VALUES . Here, nf-core has pre-populated the line params.fasta = getGenomeAttribute('fasta') . The Manta module's second input block is for the FASTA and its corresponding index file. While this getGenomeAttribute function is excellent for production runs with whole genome FASTA files from igenomes (see the dropdown below for details), for this tutorial, we'll stick with the simple example file used in the Manta test cases. Using Whole Genome FASTA File The `getGenomeAttribute` function (`def getGenomeAttribute(attribute)`) is designed to fetch genome-specific attributes. As you can see from its definition (which you can find using `git grep getGenomeAttribute`): def getGenomeAttribute(attribute) { if (params.genomes && params.genome && params.genomes.containsKey(params.genome)) { if (params.genomes[ params.genome ].containsKey(attribute)) { return params.genomes[ params.genome ][ attribute ] } } return null } This function expects two parameters, `genomes` and `genome`, to be defined. In your `nextflow.config`, you'll already find a section for references: // References genome = null igenomes_base = 's3://ngi-igenomes/igenomes/' This allows you to use `igenomes` references on AWS S3. To understand what to set for `genome`, you can consult `config/igenomes.config`, which lists various reference paths. For this example pipeline, we'll match the `Manta` test cases exactly. When we ran our Manta module test, we added the following to conf/test.config : // Input data // TODO nf-core: Specify the paths to your test data on nf-core/test-datasets modules_testdata_base_path = 'https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/' Let's adapt this parameter for our main configuration file, nextflow.config , by renaming it and adding it to the params scope. Paste the following: // Input options manta_input_data_base_path = 'https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/'","title":"Defining Input Data Paths"},{"location":"user-guide/step5-build-a-one-step-workflow/#creating-input-channels-in-mainnf","text":"Now, let's update our main.nf script by adding our inputs as channels . Nextflow is built on a dataflow programming model, where processes communicate through channels, which are fundamental for moving data through your pipeline. You can learn more about them here: \ud83d\udd17 https://www.nextflow.io/docs/latest/channel.html Add the following CHANNEL DEFINITIONS section to your main.nf script: /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CHANNEL DEFINITIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ // Channel for the first input: tuple val(meta), path(input), path(index), path(target_bed), path(target_bed_tbi) Channel.from([ tuple( [ id: 'test' ], // meta file(params.manta_input_data_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram', checkIfExists: true), // input (CRAM) file(params.manta_input_data_base_path + 'genomics/homo_sapiens/illumina/cram/test.paired_end.sorted.cram.crai', checkIfExists: true), // index (CRAI) [], // target_bed (empty list, as no BED is used for this basic test) [] // target_bed_tbi (empty list) ) ]).set { manta_main_input_ch } // This channel matches the first input signature // Channel for the second input: tuple val(meta2), path(fasta) Channel.from([ tuple( [ id: 'genome' ], // meta2 file(params.manta_input_data_base_path + 'genomics/homo_sapiens/genome/genome.fasta', checkIfExists: true) // fasta ) ]).set { manta_fasta_ch } // Channel for the third input: tuple val(meta3), path(fai) Channel.from([ tuple( [ id: 'genome' ], // meta3 file(params.manta_input_data_base_path + 'genomics/homo_sapiens/genome/genome.fasta.fai', checkIfExists: true) // fai ) ]).set { manta_fai_ch } // Channel for the fourth input: path(config) Channel .of(\"[manta]\", \"enableRemoteReadRetrievalForInsertionsInGermlineCallingModes = 0\") .collectFile(name: \"manta_options.ini\", newLine: true) .set { manta_config_ch } Essentially, we're taking the input syntax from the tests/main.nf.test script and wrapping these inputs in Nextflow channels.","title":"Creating Input Channels in main.nf"},{"location":"user-guide/step5-build-a-one-step-workflow/#calling-the-manta-module","text":"With the input channels defined, we can now call our module within the main workflow: /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ NAMED WORKFLOWS FOR PIPELINE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ workflow NCIDCEG_FLOWIQ { main: // Call MANTA_GERMLINE, feeding the channels in the correct order // as defined in the module's 'input:' block MANTA_GERMLINE( manta_main_input_ch, // Corresponds to `tuple val(meta), path(input), path(index), path(target_bed), path(target_bed_tbi)` manta_fasta_ch, // Corresponds to `tuple val(meta2), path(fasta)` manta_fai_ch, // Corresponds to `tuple val(meta3), path(fai)` manta_config_ch // Corresponds to `path(config)` ) } /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RUN MAIN WORKFLOW ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ workflow { main: NCIDCEG_FLOWIQ() } /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ THE END ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */","title":"Calling the Manta Module"},{"location":"user-guide/step5-build-a-one-step-workflow/#running-your-pipeline","text":"Now that your main.nf is updated, you can test the pipeline: nextflow run main.nf -profile conda You've now created a minimal pipeline that successfully runs the Manta module!","title":"Running Your Pipeline"},{"location":"user-guide/step5-build-a-one-step-workflow/#finishing-touches-organizing-your-pipeline","text":"Now that we have a working pipeline, let's take a moment to clean things up and follow nf-core best practices. Open your main.nf script. Near the top, you'll see a section like this: /******************************************************************************************************** IMPORT FUNCTIONS / MODULES / SUBWORKFLOWS / WORKFLOWS ********************************************************************************************************/ include { FLOWIQ } from './workflows/flowiq' This line tells us that the pipeline is importing a workflow definition from the flowiq script located in the workflows/ directory. \ud83d\udc49 The nf-core convention is to keep the root main.nf minimal and delegate all workflow logic to these included scripts. This significantly improves readability, scalability, and maintainability as your pipeline grows. Since we've added the Manta module to our pipeline, we should also relocate its include statement: include { MANTA_GERMLINE } from '../modules/nf-core/manta/germline' This import should live in workflows/flowiq.nf instead of main.nf , because that's where the logic using it will reside. In short: keep main.nf tidy and focused on high-level orchestration, and let flowiq.nf handle the details of your workflow. Once you've done that, your pipeline will not only run correctly but also follow a clean, modular design that's easier to share and maintain.","title":"Finishing Touches: Organizing Your Pipeline"},{"location":"user-guide/step5-build-a-one-step-workflow/#connecting-modules-emitting-outputs","text":"Finally, let's look at how to connect modules in a multi-step pipeline where you need to use the output from one module as input to another. The MANTA_GERMLINE module produces several outputs, as defined in its main.nf script. To make these outputs available to other processes or for further use in your workflow, you need to emit them from your workflow. Here's how you can do that within your FLOWIQ workflow: workflow FLOWIQ { main: MANTA_GERMLINE( manta_main_input_ch, // Corresponds to `tuple val(meta), path(input), path(index), path(target_bed), path(target_bed_tbi)` manta_fasta_ch, // Corresponds to `tuple val(meta2), path(fasta)` manta_fai_ch, // Corresponds to `tuple val(meta3), path(fai)` manta_config_ch // Corresponds to `path(config)` ) emit: candidate_small_indels_vcf = MANTA_GERMLINE.out.candidate_small_indels_vcf candidate_small_indels_vcf_tbi = MANTA_GERMLINE.out.candidate_small_indels_vcf_tbi candidate_sv_vcf = MANTA_GERMLINE.out.candidate_sv_vcf candidate_sv_vcf_tbi = MANTA_GERMLINE.out.candidate_sv_vcf_tbi diploid_sv_vcf = MANTA_GERMLINE.out.diploid_sv_vcf diploid_sv_vcf_tbi = MANTA_GERMLINE.out.diploid_sv_vcf_tbi versions = MANTA_GERMLINE.out.versions } We obtain the names of the output variables directly from the output: block of the manta/germline/main.nf script (e.g., emit: candidate_small_indels_vcf ). These are then assigned to new variables within the emit block of our FLOWIQ workflow. To verify that everything is working as expected, you can include a simple line in your workflow to print the contents of an output channel: candidate_small_indels_vcf.view() You can also inspect the Directed Acyclic Graph (DAG) generated in the output directory specified by params.outdir in nextflow.config . It provides a visual overview of the data flow between processes. For example: This helps confirm the structure and behavior of your pipeline by showing how data moves through each channel. Note: The main.nf and workflows/flowiq.nf scripts are included in this repository for your reference \u2014 in fact, we included the entire pipeline repository in nci-dceg-flowiq .","title":"Connecting Modules: Emitting Outputs"},{"location":"user-guide/step6-test-on-biowulf/","text":"Step 6: Test on Biowulf HPC When running on Biowulf HPC, we need to configure settings specific to that environment\u2014most importantly, specifying which executor to use. This is typically done using profiles in the nextflow.config file. A profile allows you to bundle environment-specific settings like the executor type, memory and CPU allocations, and other resource or cluster-specific configurations. You can activate a profile using the -profile option when launching a pipeline. On Biowulf, the key setting is: executor = 'slurm' Let\u2019s break down why this matters. \u2705 What happens when executor = 'slurm' is set? When you specify executor = 'slurm' , Nextflow: Submits jobs using sbatch Translates process directives ( cpus , memory , time ) into #SBATCH options Automatically manages cluster queue behavior (submission rate, polling interval, retries, etc.) Ensures each process runs on a compute node, not the local machine This setup is essential for proper resource allocation and performance on clusters like Biowulf. \u274c What happens if executor is not set? If you don\u2019t specify an executor, Nextflow defaults to: executor = 'local' Which means: All processes run on the same machine where nextflow run is executed No jobs are submitted to Slurm Resource directives like --mem , --cpus , and --time are not interpreted as SLURM constraints You\u2019re simply multithreading on a single node\u2014either in an interactive session or within an sbatch -wrapped shell This is not what you want on an HPC cluster like Biowulf. How and where are resources set? Just like the executor, resources like CPU, memory, and time are set through configuration files and Nextflow process labels. For example, in manta/germline/main.nf : process MANTA_GERMLINE { tag \"$meta.id\" label 'process_medium' label 'error_retry' } The label process_medium refers to this block in conf/base.config : withLabel:process_medium { cpus = { 6 * task.attempt } memory = { 36.GB * task.attempt } time = { 8.h * task.attempt } } This setup dynamically scales resources with retry attempts. Biowulf Support in nf-core/configs Biowulf is one of the many clusters pre-configured in the nf-core/configs repository. This means you don\u2019t have to define Biowulf-specific settings yourself. You can simply run: nextflow run main.nf -c nextflow.config -profile biowulf Nextflow will automatically pull the Biowulf configuration from: \ud83d\udd17 https://github.com/nf-core/configs/blob/master/conf/biowulf.config This profile sets executor = 'slurm' , defines staging behavior, handles file caching, and more\u2014ready to use out of the box. For more info, see: \ud83d\udd17 Biowulf-specific Nextflow docs at NIH \ud83d\udd17 nf-core/configs homepage \u26a0\ufe0f Naming Conflicts When Adding Your Own Profiles If you define your own biowulf profile in your local nextflow.config , it may conflict with the official one from nf-core/configs . That\u2019s because Nextflow prioritizes remote profiles when you use -profile . To avoid this, use a different name for custom configs: profiles { biowulf_custom { process.executor = 'slurm' // your overrides here } } Then launch with: nextflow run main.nf -c nextflow.config -profile biowulf_custom How to Execute Create a batch input file (e.g. run_manta_on_biowulf.sh ) to run the master Nextflow process. For example: #! /bin/bash #SBATCH --job-name=nextflow-main #SBATCH --cpus-per-task=4 #SBATCH --mem=4G #SBATCH --gres=lscratch:200 #SBATCH --time=24:00:00 module load nextflow export NXF_SINGULARITY_CACHEDIR=/data/$USER/nxf_singularity_cache; export SINGULARITY_CACHEDIR=/data/$USER/.singularity; export TMPDIR=/lscratch/$SLURM_JOB_ID export NXF_JVM_ARGS=\"-Xms2g -Xmx4g\" nextflow run main.nf -c nextflow.config -profile conda,biowulf Submit the job using sbatch : sbatch nf_main.sh Note: Find more details specific to using Nextflow on Biowulf HPC, see the official documentation at: https://hpc.nih.gov/apps/nextflow.html A template submission script, run_manta_on_biowulf.sh , is also included in the nci-dceg-flowiq directory of this repo for your reference.","title":"Step 6: Test on Biowulf HPC"},{"location":"user-guide/step6-test-on-biowulf/#step-6-test-on-biowulf-hpc","text":"When running on Biowulf HPC, we need to configure settings specific to that environment\u2014most importantly, specifying which executor to use. This is typically done using profiles in the nextflow.config file. A profile allows you to bundle environment-specific settings like the executor type, memory and CPU allocations, and other resource or cluster-specific configurations. You can activate a profile using the -profile option when launching a pipeline. On Biowulf, the key setting is: executor = 'slurm' Let\u2019s break down why this matters.","title":"Step 6: Test on Biowulf HPC"},{"location":"user-guide/step6-test-on-biowulf/#what-happens-when-executor-slurm-is-set","text":"When you specify executor = 'slurm' , Nextflow: Submits jobs using sbatch Translates process directives ( cpus , memory , time ) into #SBATCH options Automatically manages cluster queue behavior (submission rate, polling interval, retries, etc.) Ensures each process runs on a compute node, not the local machine This setup is essential for proper resource allocation and performance on clusters like Biowulf.","title":"\u2705 What happens when executor = 'slurm' is set?"},{"location":"user-guide/step6-test-on-biowulf/#what-happens-if-executor-is-not-set","text":"If you don\u2019t specify an executor, Nextflow defaults to: executor = 'local' Which means: All processes run on the same machine where nextflow run is executed No jobs are submitted to Slurm Resource directives like --mem , --cpus , and --time are not interpreted as SLURM constraints You\u2019re simply multithreading on a single node\u2014either in an interactive session or within an sbatch -wrapped shell This is not what you want on an HPC cluster like Biowulf.","title":"\u274c What happens if executor is not set?"},{"location":"user-guide/step6-test-on-biowulf/#how-and-where-are-resources-set","text":"Just like the executor, resources like CPU, memory, and time are set through configuration files and Nextflow process labels. For example, in manta/germline/main.nf : process MANTA_GERMLINE { tag \"$meta.id\" label 'process_medium' label 'error_retry' } The label process_medium refers to this block in conf/base.config : withLabel:process_medium { cpus = { 6 * task.attempt } memory = { 36.GB * task.attempt } time = { 8.h * task.attempt } } This setup dynamically scales resources with retry attempts.","title":"How and where are resources set?"},{"location":"user-guide/step6-test-on-biowulf/#biowulf-support-in-nf-coreconfigs","text":"Biowulf is one of the many clusters pre-configured in the nf-core/configs repository. This means you don\u2019t have to define Biowulf-specific settings yourself. You can simply run: nextflow run main.nf -c nextflow.config -profile biowulf Nextflow will automatically pull the Biowulf configuration from: \ud83d\udd17 https://github.com/nf-core/configs/blob/master/conf/biowulf.config This profile sets executor = 'slurm' , defines staging behavior, handles file caching, and more\u2014ready to use out of the box. For more info, see: \ud83d\udd17 Biowulf-specific Nextflow docs at NIH \ud83d\udd17 nf-core/configs homepage","title":"Biowulf Support in nf-core/configs"},{"location":"user-guide/step6-test-on-biowulf/#naming-conflicts-when-adding-your-own-profiles","text":"If you define your own biowulf profile in your local nextflow.config , it may conflict with the official one from nf-core/configs . That\u2019s because Nextflow prioritizes remote profiles when you use -profile . To avoid this, use a different name for custom configs: profiles { biowulf_custom { process.executor = 'slurm' // your overrides here } } Then launch with: nextflow run main.nf -c nextflow.config -profile biowulf_custom","title":"\u26a0\ufe0f Naming Conflicts When Adding Your Own Profiles"},{"location":"user-guide/step6-test-on-biowulf/#how-to-execute","text":"Create a batch input file (e.g. run_manta_on_biowulf.sh ) to run the master Nextflow process. For example: #! /bin/bash #SBATCH --job-name=nextflow-main #SBATCH --cpus-per-task=4 #SBATCH --mem=4G #SBATCH --gres=lscratch:200 #SBATCH --time=24:00:00 module load nextflow export NXF_SINGULARITY_CACHEDIR=/data/$USER/nxf_singularity_cache; export SINGULARITY_CACHEDIR=/data/$USER/.singularity; export TMPDIR=/lscratch/$SLURM_JOB_ID export NXF_JVM_ARGS=\"-Xms2g -Xmx4g\" nextflow run main.nf -c nextflow.config -profile conda,biowulf Submit the job using sbatch : sbatch nf_main.sh Note: Find more details specific to using Nextflow on Biowulf HPC, see the official documentation at: https://hpc.nih.gov/apps/nextflow.html A template submission script, run_manta_on_biowulf.sh , is also included in the nci-dceg-flowiq directory of this repo for your reference.","title":"How to Execute"},{"location":"user-guide/step7-prepare-for-aws-healthomics/","text":"Step 7: Prepare for AWS HealthOmics Flow-IQ Explorer Flow-IQ is an interactive toolkit designed to help users transition from the Biowulf HPC environment to the cloud. It maps Biowulf environment modules to equivalent Docker images and includes curated links to cloud-accessible datasets (e.g., AWS Open Data equivalents of Biowulf iGenomes). Here\u2019s a short clip demonstrating how it works: Validate with Linters To ensure that your Nextflow pipeline is cloud-ready and compatible with AWS HealthOmics, we use two linters: Tool What It Checks Why Use It linter-rules-for-nextflow Basic Nextflow syntax and AWS HealthOmics-specific requirements Fast, lightweight checks for syntax and compatibility nf-core/tools Conformance with nf-core guidelines and community best practices Thorough checks to ensure reproducibility and shareability You can run both tools to validate your pipeline against key technical and community standards. linter-rules-for-nextflow AWS originally developed an open-source tool called linter-rules-for-nextflow to catch issues in Nextflow pipelines before runtime. We\u2019ve created a customized fork of this tool, added some rules tailored for NIH researchers transitioning from Biowulf to AWS HealthOmics, and developed a script so that the Docker version of this tool can be used in the Biowulf HPC environment. To make it usable in HPC environments that support Apptainer (but not Docker), we provide a helper script: docker_to_apptainer_nextflow_linter.sh This script converts the Docker image into an Apptainer-compatible format and runs the linter seamlessly in environments like Biowulf. Features: Validates general Nextflow syntax Checks for AWS HealthOmics-specific requirements Verifies configuration files Provides clear violation messages with suggestions Runs without Docker via Apptainer Easy-to-follow tutorial and documentation available in the Flow-IQ/scripts folder nf-core pipelines lint The nf-core project defines best-practice standards for building and sharing Nextflow pipelines. The developed nf-core/tools Python package includes which includes a Nextflow linter which checks for syntax errors as well as comparing the pipeline structure against nf-core community guidelines. Why use it: Enforces nf-core standards for reproducibility Helps prepare pipelines for broader sharing or publication Easy to install and run Resources: Usage documentation: nf-core tools lint Full guidelines list: nf-core pipeline guidelines Using these tools prepares your pipeline for the next step: Deploy to AWS HealthOmics ! \ud83d\ude80","title":"Step 7: Prepare for AWS HealthOmics"},{"location":"user-guide/step7-prepare-for-aws-healthomics/#step-7-prepare-for-aws-healthomics","text":"","title":"Step 7: Prepare for AWS HealthOmics"},{"location":"user-guide/step7-prepare-for-aws-healthomics/#flow-iq-explorer","text":"Flow-IQ is an interactive toolkit designed to help users transition from the Biowulf HPC environment to the cloud. It maps Biowulf environment modules to equivalent Docker images and includes curated links to cloud-accessible datasets (e.g., AWS Open Data equivalents of Biowulf iGenomes). Here\u2019s a short clip demonstrating how it works:","title":"Flow-IQ Explorer"},{"location":"user-guide/step7-prepare-for-aws-healthomics/#validate-with-linters","text":"To ensure that your Nextflow pipeline is cloud-ready and compatible with AWS HealthOmics, we use two linters: Tool What It Checks Why Use It linter-rules-for-nextflow Basic Nextflow syntax and AWS HealthOmics-specific requirements Fast, lightweight checks for syntax and compatibility nf-core/tools Conformance with nf-core guidelines and community best practices Thorough checks to ensure reproducibility and shareability You can run both tools to validate your pipeline against key technical and community standards.","title":"Validate with Linters"},{"location":"user-guide/step7-prepare-for-aws-healthomics/#linter-rules-for-nextflow","text":"AWS originally developed an open-source tool called linter-rules-for-nextflow to catch issues in Nextflow pipelines before runtime. We\u2019ve created a customized fork of this tool, added some rules tailored for NIH researchers transitioning from Biowulf to AWS HealthOmics, and developed a script so that the Docker version of this tool can be used in the Biowulf HPC environment. To make it usable in HPC environments that support Apptainer (but not Docker), we provide a helper script: docker_to_apptainer_nextflow_linter.sh This script converts the Docker image into an Apptainer-compatible format and runs the linter seamlessly in environments like Biowulf. Features: Validates general Nextflow syntax Checks for AWS HealthOmics-specific requirements Verifies configuration files Provides clear violation messages with suggestions Runs without Docker via Apptainer Easy-to-follow tutorial and documentation available in the Flow-IQ/scripts folder","title":"linter-rules-for-nextflow"},{"location":"user-guide/step7-prepare-for-aws-healthomics/#nf-core-pipelines-lint","text":"The nf-core project defines best-practice standards for building and sharing Nextflow pipelines. The developed nf-core/tools Python package includes which includes a Nextflow linter which checks for syntax errors as well as comparing the pipeline structure against nf-core community guidelines. Why use it: Enforces nf-core standards for reproducibility Helps prepare pipelines for broader sharing or publication Easy to install and run Resources: Usage documentation: nf-core tools lint Full guidelines list: nf-core pipeline guidelines Using these tools prepares your pipeline for the next step: Deploy to AWS HealthOmics ! \ud83d\ude80","title":"nf-core pipelines lint"},{"location":"user-guide/step8-deploy-to-aws-healthomics/","text":"Step 8: Deploy to AWS HealthOmics This step walks you through how to deploy a workflow to AWS HealthOmics , focusing on setting up your containers, uploading your workflow and parameters, and running it in the cloud. Start by visiting the AWS HealthOmics landing page: AWS HealthOmics provides three main services: Storage Workflows Analytics For our purposes, we\u2019ll focus on Workflows . Select Create workflows under the Bioinformatics workflows section: On the next screen, the How it works panel outlines the four major steps to create and run a workflow: \ud83d\udce6 Container Requirements A key requirement for using AWS HealthOmics is that all containers used in workflows must be stored in Amazon Elastic Container Registry (ECR) \u2014 AWS\u2019s managed Docker image service. Public registries like quay.io or Docker Hub are not supported directly. In our case, the container required by a module is defined in manta/germline/main.nf : container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/manta:1.6.0--h9ee0642_1' : 'biocontainers/manta:1.6.0--h9ee0642_1' }\" We also see in nextflow.config : docker.registry = 'quay.io' This means we need to pull the image locally from quay.io and push it to a private ECR repository so HealthOmics can access it. Step 1: Upload the Container to Private ECR Navigate to Amazon ECR > Private repositories and click Create repository . For the name, use something like biocontainers/manta . Once created, go to the repository and click View push commands . This provides the full set of docker and awscli commands needed to push your image: # Authenticate with ECR $ aws ecr get-login-password --region us-east-1 \\ | docker login --username AWS \\ --password-stdin 222634394918.dkr.ecr.us-east-1.amazonaws.com # Pull from quay.io $ docker pull quay.io/biocontainers/manta:1.6.0--h9ee0642_1 # Tag and push to your private ECR $ docker tag quay.io/biocontainers/manta:1.6.0--h9ee0642_1 \\ 222634394918.dkr.ecr.us-east-1.amazonaws.com/biocontainers/manta:1.6.0--h9ee0642_1 $ docker push 222634394918.dkr.ecr.us-east-1.amazonaws.com/biocontainers/manta:1.6.0--h9ee0642_1 Step 2: Prepare Workflow Definition and Parameters HealthOmics requires: A workflow definition (e.g., a Nextflow script) A JSON file containing the input parameters You already have the Nextflow pipeline; now you need to generate the corresponding parameter file. Use the nf-core CLI to do this: nf-core pipelines --create-params-file # or nf-core pipelines --launch Both methods generate a nf-params.json file, which you can upload directly to HealthOmics. Step 3: Create the Workflow Click Create workflow in the HealthOmics Workflows section: In the setup wizard, provide: A workflow name and description The workflow language (e.g., Nextflow) Path to the script (local file or S3) Your parameter file ( nf-params.json ) \u2014 recommended to avoid manual errors You\u2019ll also be asked to define: Parameter types (required, optional) Descriptions Any additional custom parameters Once configured, click Create workflow . You\u2019ll be returned to the dashboard and can now proceed to run the workflow. Step 4: Start a Run You\u2019re now ready to run the workflow! Click Start run and complete the wizard. One important detail: You must provide a Service Role during this step. This role allows HealthOmics to interact with other AWS resources like S3 buckets. If you don\u2019t have permission to create IAM roles, you\u2019ll need help from an AWS admin. See: Service roles for AWS HealthOmics \ud83d\udcca Job Monitoring After launching a run, HealthOmics provides tools to monitor job status, review logs, and inspect output artifacts directly through the dashboard. \ud83d\udee0\ufe0f Automation Tip All of these steps \u2014 uploading containers, defining workflows, starting runs \u2014 can be automated using the AWS CLI or SDKs. This is useful for CI/CD or scaling to many analyses. For example, you can create a workflow using the AWS CLI with: aws omics create-workflow \\ --name flowiq-manta \\ --engine Nextflow \\ --definition-zip fileb://nci-dceg-flowiq.zip \\ --parameter-template file://nf-params.json You can also start a run for an existing workflow: aws omics start-run \\ --workflow-id 7569906 \\ --role-arn arn:aws:iam::123456789012:role/omics-service-role-serviceRole-W8O1XMPL7QZ \\ --name 'cram-to-bam' \\ --output-uri s3://omics-artifacts-01d6xmpl4e72dd32/workflow-output/ \\ --run-group-id 1234567 \\ --priority 1 \\ --storage-capacity 10 \\ --log-level ALL \\ --parameters file://nf-params.json These commands allow for seamless integration of HealthOmics workflows into automated data pipelines and reproducible cloud workflows.","title":"Step 8: Deploy to AWS HealthOmics"},{"location":"user-guide/step8-deploy-to-aws-healthomics/#step-8-deploy-to-aws-healthomics","text":"This step walks you through how to deploy a workflow to AWS HealthOmics , focusing on setting up your containers, uploading your workflow and parameters, and running it in the cloud. Start by visiting the AWS HealthOmics landing page: AWS HealthOmics provides three main services: Storage Workflows Analytics For our purposes, we\u2019ll focus on Workflows . Select Create workflows under the Bioinformatics workflows section: On the next screen, the How it works panel outlines the four major steps to create and run a workflow:","title":"Step 8: Deploy to AWS HealthOmics"},{"location":"user-guide/step8-deploy-to-aws-healthomics/#container-requirements","text":"A key requirement for using AWS HealthOmics is that all containers used in workflows must be stored in Amazon Elastic Container Registry (ECR) \u2014 AWS\u2019s managed Docker image service. Public registries like quay.io or Docker Hub are not supported directly. In our case, the container required by a module is defined in manta/germline/main.nf : container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/manta:1.6.0--h9ee0642_1' : 'biocontainers/manta:1.6.0--h9ee0642_1' }\" We also see in nextflow.config : docker.registry = 'quay.io' This means we need to pull the image locally from quay.io and push it to a private ECR repository so HealthOmics can access it.","title":"\ud83d\udce6 Container Requirements"},{"location":"user-guide/step8-deploy-to-aws-healthomics/#step-1-upload-the-container-to-private-ecr","text":"Navigate to Amazon ECR > Private repositories and click Create repository . For the name, use something like biocontainers/manta . Once created, go to the repository and click View push commands . This provides the full set of docker and awscli commands needed to push your image: # Authenticate with ECR $ aws ecr get-login-password --region us-east-1 \\ | docker login --username AWS \\ --password-stdin 222634394918.dkr.ecr.us-east-1.amazonaws.com # Pull from quay.io $ docker pull quay.io/biocontainers/manta:1.6.0--h9ee0642_1 # Tag and push to your private ECR $ docker tag quay.io/biocontainers/manta:1.6.0--h9ee0642_1 \\ 222634394918.dkr.ecr.us-east-1.amazonaws.com/biocontainers/manta:1.6.0--h9ee0642_1 $ docker push 222634394918.dkr.ecr.us-east-1.amazonaws.com/biocontainers/manta:1.6.0--h9ee0642_1","title":"Step 1: Upload the Container to Private ECR"},{"location":"user-guide/step8-deploy-to-aws-healthomics/#step-2-prepare-workflow-definition-and-parameters","text":"HealthOmics requires: A workflow definition (e.g., a Nextflow script) A JSON file containing the input parameters You already have the Nextflow pipeline; now you need to generate the corresponding parameter file. Use the nf-core CLI to do this: nf-core pipelines --create-params-file # or nf-core pipelines --launch Both methods generate a nf-params.json file, which you can upload directly to HealthOmics.","title":"Step 2: Prepare Workflow Definition and Parameters"},{"location":"user-guide/step8-deploy-to-aws-healthomics/#step-3-create-the-workflow","text":"Click Create workflow in the HealthOmics Workflows section: In the setup wizard, provide: A workflow name and description The workflow language (e.g., Nextflow) Path to the script (local file or S3) Your parameter file ( nf-params.json ) \u2014 recommended to avoid manual errors You\u2019ll also be asked to define: Parameter types (required, optional) Descriptions Any additional custom parameters Once configured, click Create workflow . You\u2019ll be returned to the dashboard and can now proceed to run the workflow.","title":"Step 3: Create the Workflow"},{"location":"user-guide/step8-deploy-to-aws-healthomics/#step-4-start-a-run","text":"You\u2019re now ready to run the workflow! Click Start run and complete the wizard. One important detail: You must provide a Service Role during this step. This role allows HealthOmics to interact with other AWS resources like S3 buckets. If you don\u2019t have permission to create IAM roles, you\u2019ll need help from an AWS admin. See: Service roles for AWS HealthOmics","title":"Step 4: Start a Run"},{"location":"user-guide/step8-deploy-to-aws-healthomics/#job-monitoring","text":"After launching a run, HealthOmics provides tools to monitor job status, review logs, and inspect output artifacts directly through the dashboard.","title":"\ud83d\udcca Job Monitoring"},{"location":"user-guide/step8-deploy-to-aws-healthomics/#automation-tip","text":"All of these steps \u2014 uploading containers, defining workflows, starting runs \u2014 can be automated using the AWS CLI or SDKs. This is useful for CI/CD or scaling to many analyses. For example, you can create a workflow using the AWS CLI with: aws omics create-workflow \\ --name flowiq-manta \\ --engine Nextflow \\ --definition-zip fileb://nci-dceg-flowiq.zip \\ --parameter-template file://nf-params.json You can also start a run for an existing workflow: aws omics start-run \\ --workflow-id 7569906 \\ --role-arn arn:aws:iam::123456789012:role/omics-service-role-serviceRole-W8O1XMPL7QZ \\ --name 'cram-to-bam' \\ --output-uri s3://omics-artifacts-01d6xmpl4e72dd32/workflow-output/ \\ --run-group-id 1234567 \\ --priority 1 \\ --storage-capacity 10 \\ --log-level ALL \\ --parameters file://nf-params.json These commands allow for seamless integration of HealthOmics workflows into automated data pipelines and reproducible cloud workflows.","title":"\ud83d\udee0\ufe0f Automation Tip"}]}